# Tightwad cluster topology
# Edit this to match your hardware

coordinator:
  host: 0.0.0.0  # ROCm machine serves the API
  port: 8080
  backend: hip  # hip or cuda
  gpus:
    - name: "7900 XTX #0"
      vram_gb: 24
    - name: "7900 XTX #1"
      vram_gb: 24

workers:
  - host: 192.168.1.100  # Windows desktop
    gpus:
      - name: "RTX 4070 Ti Super"
        vram_gb: 16
        rpc_port: 50052
      - name: "RTX 3060"
        vram_gb: 12
        rpc_port: 50053

models:
  qwen3-72b:
    path: /models/Qwen3-72B-Q4_K_M.gguf
    ctx_size: 8192
    predict: 4096
    flash_attn: true
    default: true

  deepseek-r1-70b:
    path: /models/DeepSeek-R1-Distill-Qwen-70B-Q4_K_M.gguf
    ctx_size: 8192
    predict: 4096
    flash_attn: true

  llama3.3-70b:
    path: /models/Llama-3.3-70B-Instruct-Q4_K_M.gguf
    ctx_size: 8192
    predict: 4096
    flash_attn: true

# Speculative decoding proxy â€” draft model proposes, target verifies
proxy:
  host: 0.0.0.0
  port: 8088
  max_draft_tokens: 8
  fallback_on_draft_failure: true
  draft:
    url: http://192.168.1.101:11434
    model_name: qwen3:8b
    backend: ollama
  target:
    url: http://192.168.1.100:11434
    model_name: qwen3:32b
    backend: ollama

# llama-server binary paths (per-platform)
binaries:
  coordinator: /usr/local/bin/llama-server  # ROCm build
  rpc_server: rpc-server.exe  # Windows, expected in PATH or full path
