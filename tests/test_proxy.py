"""Tests for the speculative decoding proxy."""

import json
from unittest.mock import AsyncMock, MagicMock, patch

import pytest
import pytest_asyncio
from starlette.testclient import TestClient

from tightwad.config import ProxyConfig, ServerEndpoint
from tightwad.proxy import SpeculativeProxy, apply_chat_template, create_app
from tightwad.speculation import DraftToken


@pytest.fixture
def proxy_config():
    return ProxyConfig(
        draft=ServerEndpoint(url="http://draft:8081", model_name="qwen3-8b"),
        target=ServerEndpoint(url="http://target:8080", model_name="qwen3-72b"),
        host="0.0.0.0",
        port=8088,
        max_draft_tokens=8,
        fallback_on_draft_failure=True,
    )


class TestChatTemplate:
    def test_basic_user_message(self):
        messages = [{"role": "user", "content": "Hello"}]
        prompt = apply_chat_template(messages)
        assert "<|im_start|>user\nHello<|im_end|>" in prompt
        assert "<|im_start|>assistant\n" in prompt
        assert prompt.endswith("<|im_start|>assistant\n")

    def test_system_message(self):
        messages = [
            {"role": "system", "content": "You are a pirate."},
            {"role": "user", "content": "Hi"},
        ]
        prompt = apply_chat_template(messages)
        assert "<|im_start|>system\nYou are a pirate.<|im_end|>" in prompt
        # System should not appear as a regular message
        assert prompt.count("<|im_start|>system") == 1

    def test_multi_turn(self):
        messages = [
            {"role": "user", "content": "What is 2+2?"},
            {"role": "assistant", "content": "4"},
            {"role": "user", "content": "And 3+3?"},
        ]
        prompt = apply_chat_template(messages)
        assert "<|im_start|>user\nWhat is 2+2?<|im_end|>" in prompt
        assert "<|im_start|>assistant\n4<|im_end|>" in prompt
        assert "<|im_start|>user\nAnd 3+3?<|im_end|>" in prompt

    def test_default_system_prompt(self):
        messages = [{"role": "user", "content": "Hi"}]
        prompt = apply_chat_template(messages)
        assert "You are a helpful assistant." in prompt


class TestProxyApp:
    def test_models_endpoint(self, proxy_config):
        app = create_app(proxy_config)
        client = TestClient(app)
        resp = client.get("/v1/models")
        assert resp.status_code == 200
        data = resp.json()
        assert data["object"] == "list"
        assert len(data["data"]) == 2
        assert data["data"][0]["id"] == "qwen3-72b"
        assert data["data"][1]["id"] == "qwen3-8b"

    def test_status_endpoint_servers_down(self, proxy_config):
        """When draft/target are unreachable, status shows not alive."""
        app = create_app(proxy_config)
        client = TestClient(app)
        resp = client.get("/v1/tightwad/status")
        assert resp.status_code == 200
        data = resp.json()
        assert data["draft"]["model"] == "qwen3-8b"
        assert data["target"]["model"] == "qwen3-72b"
        assert data["draft"]["health"]["alive"] is False
        assert data["target"]["health"]["alive"] is False
        assert data["stats"]["total_rounds"] == 0
        assert data["stats"]["acceptance_rate"] == 0.0

    def test_completion_draft_unavailable_fallback(self, proxy_config):
        """When both servers are down, request should fail gracefully."""
        app = create_app(proxy_config)
        client = TestClient(app, raise_server_exceptions=False)
        resp = client.post("/v1/completions", json={
            "prompt": "Hello",
            "max_tokens": 10,
        })
        # Should get a 500 since target is also unreachable
        assert resp.status_code == 500


class TestLogprobsVerification:
    @pytest.fixture
    def llamacpp_config(self):
        return ProxyConfig(
            draft=ServerEndpoint(url="http://draft:8081", model_name="qwen3-8b", backend="llamacpp"),
            target=ServerEndpoint(url="http://target:8080", model_name="qwen3-72b", backend="llamacpp"),
            host="0.0.0.0",
            port=8088,
            max_draft_tokens=4,
        )

    def test_can_use_logprobs_llamacpp(self, llamacpp_config):
        proxy = SpeculativeProxy(llamacpp_config)
        assert proxy._can_use_logprobs() is True

    def test_cannot_use_logprobs_ollama(self):
        config = ProxyConfig(
            draft=ServerEndpoint(url="http://draft:11434", model_name="qwen3-8b", backend="ollama"),
            target=ServerEndpoint(url="http://target:11434", model_name="qwen3-32b", backend="ollama"),
        )
        proxy = SpeculativeProxy(config)
        assert proxy._can_use_logprobs() is False

    def test_cannot_use_logprobs_mixed(self):
        config = ProxyConfig(
            draft=ServerEndpoint(url="http://draft:11434", model_name="qwen3-8b", backend="ollama"),
            target=ServerEndpoint(url="http://target:8080", model_name="qwen3-72b", backend="llamacpp"),
        )
        proxy = SpeculativeProxy(config)
        assert proxy._can_use_logprobs() is False

    @pytest.mark.asyncio
    async def test_verify_with_logprobs_all_accepted(self, llamacpp_config):
        """When target agrees with all draft tokens, get accepted + bonus."""
        proxy = SpeculativeProxy(llamacpp_config)

        draft_tokens = [
            DraftToken(token_id=100, logprob=-0.1, text="The"),
            DraftToken(token_id=200, logprob=-0.2, text=" answer"),
            DraftToken(token_id=300, logprob=-0.15, text=" is"),
        ]

        # Mock target response: echo=true returns logprobs for all tokens
        # Simulate: 5 prompt tokens + 3 draft tokens + 1 bonus = 9 content entries
        mock_content = [
            # 5 prompt token entries (we don't care about these)
            {"id": 1, "token": "<|im", "logprob": -0.01},
            {"id": 2, "token": "_start", "logprob": -0.01},
            {"id": 3, "token": "|>", "logprob": -0.01},
            {"id": 4, "token": "system", "logprob": -0.01},
            {"id": 5, "token": "\n", "logprob": -0.01},
            # 3 draft token positions â€” target agrees (same token_id)
            {"id": 100, "token": "The", "logprob": -0.05},
            {"id": 200, "token": " answer", "logprob": -0.1},
            {"id": 300, "token": " is", "logprob": -0.08},
            # 1 bonus token
            {"id": 400, "token": " 42", "logprob": -0.3},
        ]

        mock_resp = MagicMock()
        mock_resp.json.return_value = {
            "choices": [{"logprobs": {"content": mock_content}}]
        }

        proxy.target_client.post = AsyncMock(return_value=mock_resp)

        result = await proxy.verify_with_logprobs("prompt text", draft_tokens, temperature=0.0)

        assert result.accepted_count == 3
        assert result.rejected_at is None
        assert result.bonus_token is not None
        assert result.bonus_token.token_id == 400

        await proxy.close()

    @pytest.mark.asyncio
    async def test_verify_with_logprobs_partial_accept(self, llamacpp_config):
        """When target disagrees at position 2, accept first 2 and resample."""
        proxy = SpeculativeProxy(llamacpp_config)

        draft_tokens = [
            DraftToken(token_id=100, logprob=-0.1, text="The"),
            DraftToken(token_id=200, logprob=-0.2, text=" answer"),
            DraftToken(token_id=300, logprob=-0.15, text=" is"),
        ]

        mock_content = [
            # 5 prompt tokens
            {"id": 1, "token": "a", "logprob": -0.01},
            {"id": 2, "token": "b", "logprob": -0.01},
            {"id": 3, "token": "c", "logprob": -0.01},
            {"id": 4, "token": "d", "logprob": -0.01},
            {"id": 5, "token": "e", "logprob": -0.01},
            # Draft positions: agree, agree, DISAGREE
            {"id": 100, "token": "The", "logprob": -0.05},
            {"id": 200, "token": " answer", "logprob": -0.1},
            {"id": 999, "token": " was", "logprob": -0.08},  # target says 999, draft says 300
            # Bonus
            {"id": 400, "token": " 42", "logprob": -0.3},
        ]

        mock_resp = MagicMock()
        mock_resp.json.return_value = {
            "choices": [{"logprobs": {"content": mock_content}}]
        }

        proxy.target_client.post = AsyncMock(return_value=mock_resp)

        result = await proxy.verify_with_logprobs("prompt", draft_tokens, temperature=0.0)

        assert result.accepted_count == 2
        assert result.rejected_at == 2
        assert result.resample_token is not None
        assert result.resample_token.token_id == 999

        await proxy.close()


class TestSSEFormat:
    def test_sse_chunk_format(self):
        """SSE data lines should be valid JSON."""
        line = 'data: {"id":"cmpl-tightwad-1","object":"text_completion","choices":[{"index":0,"text":"hello","finish_reason":null}]}'
        assert line.startswith("data: ")
        payload = json.loads(line[6:])
        assert payload["object"] == "text_completion"
        assert payload["choices"][0]["text"] == "hello"

    def test_sse_done_marker(self):
        line = "data: [DONE]"
        assert line == "data: [DONE]"
